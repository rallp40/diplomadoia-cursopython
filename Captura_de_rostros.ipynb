{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Captura de rostros.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewc2GCmLrUGl"
      },
      "source": [
        "# 1) Captura de Rostros desde los videos\n",
        "\n",
        "Como primer paso se procede a enlazar la carpeta de drive que contiene los videos y las imagenes que se van a usar para entrenar al modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG1LrPVeatjA",
        "outputId": "c2823c86-14f4-4960-9c0c-41907e1d1783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sQvD0DQDLaz"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import imutils\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JiDn_KzoOrt"
      },
      "source": [
        "La función capturar_rostros recibe un nombre y un vídeo (mp4) como parametros. Crear una carpeta con el nombre de la persona y guarda todos los rostros detectados en cada frame del video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at8svWLIYGw2"
      },
      "source": [
        "def capturar_rostros(url_video,nombre_persona,rotacion,cantidad_rostros,nro_video):\n",
        "\n",
        "  #En ruta_datos se guarda la ruta de la carpeta donde se almacenaran los rostros para entrenar al modelo  \n",
        "  ruta_datos = '/content/drive/My Drive/Datos Diplomado/Aplicacion Rostros/Personas'\n",
        "  #En ruta persona se guarda la ruta donde se almacenaran los datos de la persona especifica\n",
        "  ruta_persona = ruta_datos + '/' + nombre_persona\n",
        "  #Se verifica si existe la ruta de la persona sino se crea la ruta\n",
        "  if not os.path.exists(ruta_persona):\n",
        "\t  print('Carpeta creada: ',ruta_persona)\n",
        "\t  os.makedirs(ruta_persona)\n",
        "  \n",
        "  #Este codigo es para realizar un video en directo\n",
        "  #video = cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
        "\n",
        "  #Se carga el video que se indica en url_video\n",
        "  video = cv2.VideoCapture(url_video)\n",
        "  #se usa el algoritmo CascadeClassifier para detectar los rostros\n",
        "  clasificador = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
        "  #contador de rostros detectados\n",
        "  nro_rostros = 0\n",
        "  #recuperar cuadro por cuadro del video\n",
        "  while True:\n",
        "      ret, cuadro = video.read()\n",
        "      if ret == False: break\n",
        "      #se ha detectado que algunos videos se cargan rotados 90 o 180 grados por lo que habria que girar la imagen\n",
        "      cuadro =  cv2.rotate(cuadro,rotacion)\n",
        "      #Se redimensiona el ancho del fotograma del video\n",
        "      cuadro =  imutils.resize(cuadro, width=640)\n",
        "      #Se obtiene un fotograma(cuadro) en escala de grises\n",
        "      imagen_gris = cv2.cvtColor(cuadro, cv2.COLOR_BGR2GRAY)\n",
        "      cuadroaux = cuadro.copy()\n",
        "      #se detectan todos los rostros en la imagen en gris\n",
        "      rostros = clasificador.detectMultiScale(imagen_gris,1.3,5)\n",
        "      # Por cada rostro detectado se dibuja unrectangulo\n",
        "      for (x,y,w,h) in rostros:\n",
        "          cv2.rectangle(cuadro, (x,y),(x+w,y+h),(0,255,0),2)\n",
        "          #se recorta el rostro de la imagen, se redimensiona para que todos tengan el mismo tamaño\n",
        "          #y se guarda como un archivo jpg (rostro_<nro_rostor>.jpg)\n",
        "          rostro = cuadroaux[y:y+h,x:x+w]\n",
        "          rostro = cv2.resize(rostro,(150,150),interpolation=cv2.INTER_CUBIC)\n",
        "          cv2.imwrite(ruta_persona + '/rostro_'+str(nro_video)+'_'+str(nro_rostros)+'.jpg',rostro)\n",
        "          nro_rostros = nro_rostros + 1\n",
        "      #cada 60 rostros se muestra el rostro capturado\n",
        "      if (nro_rostros % 60 == 0) and nro_rostros>1 :\n",
        "          cv2_imshow(cuadro)\n",
        "      k =  cv2.waitKey(1)\n",
        "      #El proceso termina cuando se consiguio la cantidad de rostros solicitada\n",
        "      if k == 27 or nro_rostros >= cantidad_rostros:\n",
        "          break\n",
        "  video.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ8DmCE6rS7u"
      },
      "source": [
        "# 2) Métodos para entrenar el reconocedor\n",
        "\n",
        "Los metodos usados son los siguientes:\n",
        "\n",
        "---\n",
        "**EigenFaceRecognizer** : Este emplea el método PCA (Principal Component Analysis), Según la documentación de OpenCV es: \n",
        "\n",
        "> *La idea es que un conjunto de datos de alta dimensión a menudo se describe mediante variables correlacionadas y, por lo tanto, solo unas pocas dimensiones significativas explican la mayor parte de la información.*\n",
        "\n",
        "Consideraciones:\n",
        "\n",
        "*   Las imágenes de entrenamiento como de predicción deben estar en escala de grises.\n",
        "*   El método eigenfaces asume que todas las imágenes, ya sean de entrendamiento o test deben tener el mismo tamaño.\n",
        "---\n",
        "**FisherFaceRecognizer** : Este método es una mejora de EigenFaceRecognizer \n",
        "\n",
        "Consideraciones:\n",
        "\n",
        "*   Las imágenes de entrenamiento como de predicción deben estar en escala de grises.\n",
        "*   El método eigenfaces asume que todas las imágenes, ya sean de entrendamiento o test deben tener el mismo tamaño.\n",
        "---\n",
        "**BPH (Histogramas de Patrones Binarios Locales)** : Presenta mejoras respecto a los métodos anteriores, ya que es más robusto ante cambios de iluminación. Además, citando a la documentación de OpenCV: \n",
        "\n",
        "> *La idea es no mirar la imagen completa como un vector de alta dimensión, sino describir solo las características locales de un objeto*\n",
        "\n",
        "Consideraciones:\n",
        "\n",
        "*   Las imágenes de entrenamiento como de predicción deben estar en escala de grises.\n",
        "*   No se tiene especificaciones sobre el tamaño de las imágenes correspondientes a los rostros. Por lo tanto asumimos que pueden tener distinto tamaño.\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2usDrJnxf8HP"
      },
      "source": [
        "def entrenar_modelo(modelo='LBPHFace'):\n",
        "\t#import cv2\n",
        "\t#import os\n",
        "\timport numpy as np\n",
        "\n",
        "\t#modulo=1\n",
        "\tif modelo=='EigenFaces':\n",
        "\t\tmodulo=50\n",
        "\telif modelo=='FisherFace':\n",
        "\t\tmodulo=1\n",
        "\telse:\n",
        "\t\tmodulo=10\n",
        "\n",
        "\t#ruta_datos es la carpeta raiz donde estan todas las carpetas que almacenan los rostros para utilizar en el modelo\n",
        "\truta_datos = '/content/drive/My Drive/Datos Diplomado/Aplicacion Rostros/Personas' \n",
        "\t#se crea una lista usando todas las carpetas de personas\n",
        "\tlista_personas = os.listdir(ruta_datos)\n",
        "\tprint('Lista de personas: ', lista_personas)\n",
        "\n",
        "\t#En etiquetas en este se almacenarán las etiquetas correspondientes a cada imagen según la persona.\n",
        "\tetiquetas = []\n",
        "\t# en rostros[] se almacenará cada una de las imágenes de los rostros.\n",
        "\trostros = []\n",
        "\t#identifica la persona que se esta evaluando\n",
        "\tpersona = 0\n",
        "\n",
        "\t#Con este for leemos cada directorio asignado a cada persona \n",
        "\tfor persona_directorio in lista_personas:\n",
        "\t\truta_persona = ruta_datos + '/' + persona_directorio\n",
        "\t\tprint('Leyendo las imágenes')\n",
        "\t\tcontador=0\n",
        "\t  #con este for leemos cada archivo de un directorio\n",
        "\t\tfor nombre_archivo in os.listdir(ruta_persona):\n",
        "\t\t\tif contador % modulo == 0:\n",
        "\t\t\t\t\tprint('Rostros: ', persona_directorio + '/' + nombre_archivo)\n",
        "\t    \t\t#en etiquetas añadimos la etiqueta correspondiente a la persona\n",
        "\t\t\t\t\tetiquetas.append(persona)\n",
        "\t\t\t\t\t#en rostros añadimos cada imagen de rostro de esa persona\t\n",
        "\t\t\t\t\trostros.append(cv2.imread(ruta_persona+'/'+nombre_archivo,0))\n",
        "\t\t\tcontador=contador+1\n",
        "  \t#se incrementa para guardar los datos de la siguiente persona\n",
        "\t\tpersona = persona + 1\n",
        "\n",
        "\t# se muestra las etiquetas (rostros) grabados por cada persona\n",
        "\tprint('etiquetas= ',etiquetas)\n",
        "\tfor x in range(len(lista_personas)):\n",
        "\t\tprint('Número de etiquetas :'+str(x)+'',np.count_nonzero(np.array(etiquetas)==x))\n",
        "\n",
        "  # se selecciona el modelo de acuerdo al parametro enviado\n",
        "\tif modelo=='EigenFaces':\n",
        "\t\treconocedor_rostro = cv2.face.EigenFaceRecognizer_create()\n",
        "\telif modelo=='FisherFace':\n",
        "\t\treconocedor_rostro = cv2.face.FisherFaceRecognizer_create()\n",
        "\telse:\n",
        "\t\treconocedor_rostro = cv2.face.LBPHFaceRecognizer_create()\n",
        "\n",
        "\t# se procede a entrenar el modelo enviandoles los rostros con su respectiva etiqueta\n",
        "\tprint(\"Entrenando...\")\n",
        "\treconocedor_rostro.train(rostros, np.array(etiquetas))\n",
        "\n",
        "\t# se almacena el modelo seleccionado\n",
        "\n",
        "\tif modelo=='EigenFaces':\n",
        "\t\treconocedor_rostro.write('modeloEigenFace.xml')\n",
        "\telif modelo=='FisherFace':\n",
        "\t\treconocedor_rostro.write('modeloFisherFace.xml')\n",
        "\telse:\n",
        "\t\treconocedor_rostro.write('modeloLBPHFace.xml')\n",
        "\n",
        "\tprint(\"Modelo almacenado...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rpNvLJDldOT"
      },
      "source": [
        "entrenar_modelo('LBPHFace')\n",
        "entrenar_modelo('FisherFace')\n",
        "entrenar_modelo('EigenFaces')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmw5qb_AJEzj"
      },
      "source": [
        "# 3) Reconocimientos de Rostros\n",
        "\n",
        "En este codigo se llama al modelo seleccionado para detectar un rostro conocido. Para detectar un mostro se llama a la funcion result.\n",
        "\n",
        "Esta función nos devuelve la etiqueta relacionada con la persona y un valor de confianza el cual varia de acuerdo al modelo.\n",
        "\n",
        "Se define un umbral por modelo y si el valor de confianza se encuentra por debajo de ese umbral se reconoce que el rostro detectado es correcto y corresponde a la etiqueta devuelta por el metodo predict, en caso contrario se etiqueta el rostro como desconocido\n",
        "\n",
        "Los valores de confianza que se ha definido por defecto para esta aplicacion son los siguientes:\n",
        "\n",
        "*   EigenFaces = 5700\n",
        "*   FisherFace = 500\n",
        "*   LBPHFace = 70\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDONolDnEf_D"
      },
      "source": [
        "def BuscarImagen(imagen,modelo='LBPHFace',umbral=70):\n",
        "  import cv2\n",
        "  import os\n",
        "  import requests\n",
        "  import base64\n",
        "  #se especifica la carpeta donde estan los rostros de las personas y en base a esas carpetas\n",
        "  #se crea una lista de personas\n",
        "  ruta_datos = '/content/drive/My Drive/Datos Diplomado/Aplicacion Rostros/Personas' \n",
        "  lista_personas = os.listdir(ruta_datos)\n",
        "\n",
        "  #se especifica el modelo que se va a usar\n",
        "  if modelo=='FisherFace':\n",
        "    reconocedor_rostro = cv2.face.FisherFaceRecognizer_create()\n",
        "  elif modelo=='EigenFaces':\n",
        "    reconocedor_rostro = cv2.face.EigenFaceRecognizer_create()\n",
        "  else:\n",
        "    reconocedor_rostro = cv2.face.LBPHFaceRecognizer_create()\n",
        "\n",
        "  # se lee el modelo almacenado\n",
        "  if modelo=='FisherFace':\n",
        "    reconocedor_rostro.read('modeloFisherFace.xml')\n",
        "  elif modelo=='EigenFaces':\n",
        "    reconocedor_rostro.read('modeloEigenFace.xml')\n",
        "  else:\n",
        "    reconocedor_rostro.read('modeloLBPHFace.xml')\n",
        "\n",
        "  # se llama al algoritmo para detectar rostros y se inicializa el contador de rostros\n",
        "  detector_rostro = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
        "  contador_encontro=0\n",
        "  contador_no_encontro=0\n",
        "\n",
        "  #se crea una version de la imagen en escala de grises\n",
        "  gris = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
        "  imagen_aux = gris.copy()\n",
        "\n",
        "  #en rostros se almacena todos los rostros encontrados en la imagen\n",
        "  # se inicializa encontro en false, si se detecta un rostro conocido encontro se gabrara como True\n",
        "  rostros = detector_rostro.detectMultiScale(gris,1.3,5)\n",
        "  encontro=False\n",
        "\n",
        "  #por cada rostro encontrado    \n",
        "  for (x,y,w,h) in rostros:\n",
        "          #se recorta y redimensiona el rostro\n",
        "          rostro = imagen_aux[y:y+h,x:x+w]\n",
        "          rostro = cv2.resize(rostro,(150,150),interpolation= cv2.INTER_CUBIC)\n",
        "\n",
        "          #se envia el rostro al algoritmo de prediccion y se recibe un resultado\n",
        "          resultado = reconocedor_rostro.predict(rostro)\n",
        "          #se graban los valores del resultado en la imagen\n",
        "          cv2.putText(imagen,'{}'.format(resultado),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
        "          \n",
        "          #si la distancia se encuentra por debajo del umbral se reconoce que si es la persona\n",
        "          #y se graba su nombre en la imagen, se dibuja un cuadrado \n",
        "          #y se especifica que encontro a la persona\n",
        "          if resultado[1] < umbral:\n",
        "              cv2.putText(imagen,'{}'.format(lista_personas[resultado[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)\n",
        "              cv2.rectangle(imagen, (x,y),(x+w,y+h),(0,255,0),2)\n",
        "              encontro=True\n",
        "              contador_encontro=contador_encontro+1\n",
        "          #si la distancia supera el umbral, se muestra el nombre como desconocido\n",
        "          #y se dibuja un cuadrado rojo\n",
        "          else:\n",
        "              cv2.putText(imagen,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)\n",
        "              cv2.rectangle(imagen, (x,y),(x+w,y+h),(0,0,255),2)\n",
        "              contador_no_encontro=contador_no_encontro+1\n",
        "\n",
        "  if (contador_encontro>0) or (contador_no_encontro>0):\n",
        "  \n",
        "          cv2.imwrite(\"imagen.jpg\",imagen)\n",
        "          i1 = \"/content/imagen.jpg\"\n",
        "          #codifica la imagen\n",
        "          with open(i1, \"rb\") as img_file:\n",
        "            encoded_string= base64.b64encode(img_file.read()).decode('utf-8')\n",
        "          if encontro==True:\n",
        "            r = {\"encotro\": True, \"nombre\": format(lista_personas[resultado[0]]), \"imagen\": encoded_string}\n",
        "            return r\n",
        "          else:\n",
        "            r = {\"encotro\": True, \"nombre\": \"Desconocido\", \"imagen\": encoded_string}\n",
        "            return r            \n",
        "\n",
        "  r = {\"encotro\": False, \"nombre\": \"\", \"imagen\": \"\"}\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKR16hqKFY8"
      },
      "source": [
        "# 4) Servidor Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eYyDA6sXHek",
        "outputId": "6c913086-4ce8-4574-f046-e7d63b85d1aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install flask-ngrok \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH6j55-oXZpZ"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request\n",
        "import base64\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK1KgR6Ul6-y"
      },
      "source": [
        "#inicia la aplicacion en Flask\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #inicia ngrok cuando la app esté corriendo\n",
        "@app.route(\"/who_face\", methods=['GET', 'POST'])\n",
        "def who_face():\n",
        "    #inicializa la respuesta por defecto\n",
        "    response = {\"encotro\": False, \"nombre\": \"\", \"imagen\": \"\"}\n",
        "    \n",
        "    #recupera la informacion desde la aplicacion\n",
        "    content = request.get_json(silent=True)\n",
        "    \n",
        "    #si no hay contenido devuelve la respuesta por defecto\n",
        "    if not content:\n",
        "      return jsonify(response)\n",
        "\n",
        "    #si encuentra que se envio la imagen\n",
        "    if content['imagen'] != '':\n",
        "\n",
        "      #decodifica la imagen, la transforma en una matriz y la carga con\n",
        "      #la libreria cv2\n",
        "      imgdata = base64.b64decode(content['imagen'])\n",
        "      npimg = np.fromstring(imgdata, dtype=np.uint8)\n",
        "      ProcessImage = cv2.imdecode(npimg, 1)\n",
        "  \n",
        "      #muestra la imagen y los datos recibidos desde la app    \n",
        "      cv2_imshow(ProcessImage)\n",
        "      print(content['algoritmo'])\n",
        "      print(content['confianza'])\n",
        "      \n",
        "\n",
        "      #envia la imagen recibida al algoritmo que se encargara de detectar si\n",
        "      #hay una persona en la imagen y devuelve esa imagen codificada\n",
        "      imgEncontrada = BuscarImagen(ProcessImage,content['algoritmo'],int(content['confianza']))\n",
        "\n",
        "      #se reenvia la respuesta a la App\n",
        "      response['imagen'] = imgEncontrada['imagen']\n",
        "      response['nombre'] = imgEncontrada['nombre']\n",
        "      response['encotro'] = imgEncontrada['encotro']\n",
        "\n",
        "\n",
        "    print(response, content)\n",
        "    return jsonify(response)\n",
        "app.run()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}